{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Autoencoder for jet images with  **Conv 2D**  \n",
    "\n",
    "---\n",
    "In this notebook, we train an Autoencoder on a dataset of QCD jet images.\n",
    "Both the encoder and decoder networks are based on Conv2D architectures. \n",
    "This autoencoder can be used\n",
    "- as a clustering algorithm in the latent space\n",
    "- as a compression algorithm\n",
    "- as an anomaly detection algorithm\n",
    "\n",
    "Some of these functionalities are exploited below.\n",
    "\n",
    "The notebook starts prpearing the QCD dataset and a set of\n",
    "\"anomalous jets\" (boosted W-, Z-, and top-jets).\n",
    "The AE is trained on a training+validation dataset. Then, \n",
    "it is used on the test dataset for some of the applications listed above. Due to\n",
    "the low statistics available, the validation dataset is recycled\n",
    "as a test dataset. \n",
    "\n",
    "For details on the dataset, see https://github.com/pierinim/tutorials/blob/master/HiggsSchool/Lecture1/Notebook1_ExploreDataset.ipynb\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the training and validation samples\n",
    "\n",
    "---\n",
    "In order to import the dataset, we now\n",
    "- clone the dataset repository (to import the data in Colab)\n",
    "- load the h5 files in the data/ repository\n",
    "- extract the data we need: a target and jetImage \n",
    "\n",
    "To type shell commands, we start the command line with !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not needed if running locally\n",
    "! git clone https://github.com/pierinim/tutorials.git\n",
    "! ls tutorials/Data/JetDataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "target = np.array([])\n",
    "jetImage = np.array([])\n",
    "#----------\n",
    "# if you are running locally, you should do\n",
    "# datafiles = glob.glob(\"../Data/JetDataset/*h5\") \n",
    "#----------\n",
    "# on Colab\n",
    "#----------\n",
    "datafiles = glob.glob(\"tutorials/Data/JetDataset/*h5\") \n",
    "for fileIN in datafiles:\n",
    "    print(\"Appending %s\" %fileIN)\n",
    "    f = h5py.File(fileIN, \"r\")\n",
    "    myjetImage = np.array(f.get(\"jetImage\"), np.float32)\n",
    "    mytarget = np.array(f.get('jets')[0:,-6:-1], np.float32)\n",
    "    jetImage = np.concatenate([jetImage, myjetImage], axis=0) if jetImage.size else myjetImage\n",
    "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
    "    f.close()\n",
    "print(target.shape, jetImage.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keras, images are representable as $n \\times m \\times k$ tensors, where $n \\times m$ are the pixel dimenions and $k$ is the number of channels (e.g., 1 in a black\\&while image, 3 for an RGB image). In our case, k=1. To comply to this, we add the channel index by reshaping the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetImage = jetImage.reshape((jetImage.shape[0], jetImage.shape[1], jetImage.shape[2], 1))\n",
    "print(jetImage.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now separate the dataset in 4:\n",
    "- a training dataset, consisting of quarks and gluons\n",
    "- three 'anomalous jets' samples: W, Z, and top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetImage_standard = jetImage[np.argmax(target,axis=1)<2]\n",
    "jetImage_W = jetImage[np.argmax(target,axis=1)==2]\n",
    "jetImage_Z = jetImage[np.argmax(target,axis=1)==3]\n",
    "jetImage_t = jetImage[np.argmax(target,axis=1)==4]\n",
    "print(jetImage_standard.shape, jetImage_W.shape, jetImage_Z.shape, jetImage_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is an unsupervised algorithm, so we don't need the target array anymore.\n",
    "Nevertheless, we keep a part of it around, since it might be useful to test the response \n",
    "of the algorithm to quarks and gluons separetly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_standard = target[np.argmax(target,axis=1)<2]\n",
    "del target\n",
    "del jetImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now shuffle the standard-jet data and its labels, splitting them into a training, a validation+test dataset with 2:1:1 ratio. \n",
    "\n",
    "Then we separate the validation+test in two halves (training and validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, label_train, label_val = t= train_test_split(jetImage_standard, label_standard, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, label_train.shape, label_val.shape)\n",
    "len_val = X_val.shape[0]\n",
    "X_test = X_val[int(len_val/2.):,:,:,:]\n",
    "label_test = label_val[int(len_val/2.):,:]\n",
    "X_val = X_val[:int(len_val/2.),:,:,:]\n",
    "label_test = label_val[:int(len_val/2.),:]\n",
    "print(X_train.shape, X_val.shape, X_test.shape, label_train.shape, label_val.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleanup to save memory\n",
    "del jetImage_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the ConvAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Conv2D, Flatten, Activation, Deconv2D\n",
    "from keras.layers import MaxPooling2D, BatchNormalization, Activation, Reshape, UpSampling2D\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "image_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------\n",
    "# Enncoder\n",
    "#---------\n",
    "inputImage = Input(shape=(image_shape))\n",
    "#\n",
    "x = Conv2D(10, kernel_size=(5, 5), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(inputImage)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D( pool_size = (5, 5))(x)\n",
    "#\n",
    "x = Conv2D(15, kernel_size=(4, 4), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D( pool_size = (5, 5))(x)\n",
    "#\n",
    "x = Conv2D(20, kernel_size=(4, 4), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D( pool_size = (4, 4))(x)\n",
    "#\n",
    "x = Flatten()(x)\n",
    "enc = Dense(5, activation='relu')(x)\n",
    "\n",
    "#---------\n",
    "# Decoder\n",
    "#---------\n",
    "\n",
    "x = Dense(20, activation='relu')(enc)\n",
    "x = Reshape((1, 1, 20))(x)\n",
    "#\n",
    "x = UpSampling2D((4, 4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Deconv2D(15, kernel_size=(4,4), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(x)\n",
    "x = Activation('relu')(x)\n",
    "#\n",
    "x = UpSampling2D((5, 5))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Deconv2D(10, kernel_size=(4,4), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(x)\n",
    "x = Activation('relu')(x)\n",
    "#\n",
    "x = UpSampling2D((5, 5))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Deconv2D(1, kernel_size=(4,4), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(x)\n",
    "dec = Activation('relu')(x)\n",
    "#output = Deconv2D(5, kernel_size=(5,5), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(x)\n",
    "####\n",
    "AE_enc = Model(inputs=inputImage, outputs=enc)\n",
    "AE = Model(inputs=inputImage, outputs=dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.compile(loss='mse', optimizer='adam')\n",
    "AE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "history = AE.fit(X_train, X_train, epochs=n_epochs, batch_size=batch_size, verbose = 2,\n",
    "                validation_data=(X_val, X_val),\n",
    "                callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
    "                TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.title('Training History')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "In order to detect anomalous jets, we build the distribution of the reconstruction loss (the MSE term) and define as anomaly all the jets falling in the tail, at some fixed p-value. \n",
    "To evaluate the strength of the test, we can consider a set of anomalies and evaluate the ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['W', 'Z', 'top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = [jetImage_W, jetImage_Z, jetImage_t]\n",
    "predictedQCD = AE.predict(X_test)\n",
    "predicted_anomaly = []\n",
    "for i in range(len(labels)):\n",
    "    predicted_anomaly.append(AE.predict(anomaly[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(image_in, image_out):\n",
    "    mse = (image_out-image_in)*(image_out-image_in)\n",
    "    # sum over channel\n",
    "    mse = np.sum(mse,axis=-1)\n",
    "    # sum over y\n",
    "    mse = np.sum(mse, axis=-1)\n",
    "    # sum over x\n",
    "    mse = np.sum(mse, axis=-1)\n",
    "    return mse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossQCD = mse(X_test, predictedQCD)\n",
    "loss_anomaly = []\n",
    "for i in range(len(labels)):\n",
    "    loss_anomaly.append(mse(anomaly[i], predicted_anomaly[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxScore = np.max(lossQCD)\n",
    "# plot QCD\n",
    "plt.figure()\n",
    "plt.hist(lossQCD, bins=100, label='QCD', density=True, range=(0, maxScore), \n",
    "         histtype='step', fill=False, linewidth=1.5)\n",
    "for i in range(len(labels)):\n",
    "    plt.hist(loss_anomaly[i], bins=100, label=labels[i], density=True, range=(0, maxScore),\n",
    "            histtype='step', fill=False, linewidth=1.5)\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"AE Loss\")\n",
    "plt.ylabel(\"Probability (a.u.)\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "plt.figure()\n",
    "targetQCD = np.zeros(lossQCD.shape[0])\n",
    "for i, label in enumerate(labels):\n",
    "        print(loss_anomaly[i].shape, targetQCD.shape)\n",
    "        trueVal = np.concatenate((np.ones(loss_anomaly[i].shape[0]),targetQCD))\n",
    "        predVal = np.concatenate((loss_anomaly[i],lossQCD))\n",
    "        print(trueVal.shape, predVal.shape)\n",
    "        fpr, tpr, threshold = roc_curve(trueVal,predVal)\n",
    "        auc1= auc(fpr, tpr)\n",
    "        plt.plot(tpr,fpr,label='%s Anomaly Detection, auc = %.1f%%'%(label,auc1*100.))\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "When encoding images, the AE should make alike images fall in closeby points of the bottle-neck latent space.\n",
    "The distribution in the latent space can then be used to identify in an unsupervised way groups of similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

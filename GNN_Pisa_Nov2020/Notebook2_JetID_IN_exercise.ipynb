{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Jet Tagging with **Interaction Networks** \n",
    "\n",
    "---\n",
    "In this notebook, we perform a Jet identification task using a graph-based multiclass classifier with INs.\n",
    "\n",
    "The problem consists in identifying a given jet as a quark, a gluon, a W, a Z, or a top,\n",
    "based on a jet image, i.e., a 2D histogram of the transverse momentum ($p_T$) deposited in each of 100x100\n",
    "bins of a square window of the ($\\eta$, $\\phi$) plane, centered along the jet axis.\n",
    "\n",
    "For details on the physics problem, see https://arxiv.org/pdf/1804.06913.pdf \n",
    "\n",
    "For details on the dataset, see Notebook1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.variable import *\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the training and validation samples\n",
    "\n",
    "---\n",
    "In order to import the dataset, we now\n",
    "- clone the dataset repository (to import the data in Colab)\n",
    "- load the h5 files in the data/ repository\n",
    "- extract the data we need: a target and jetImage \n",
    "\n",
    "To type shell commands, we start the command line with !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/pierinim/tutorials.git\n",
    "! ls tutorials/Data/JetDataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array([])\n",
    "jetList = np.array([])\n",
    "# we cannot load all data on Colab. So we just take a few files\n",
    "datafiles = ['tutorials/Data/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
    "           'tutorials/Data/JetDataset/jetImage_7_100p_60000_70000.h5',\n",
    "            'tutorials/Data/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
    "            'tutorials/Data/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
    "            'tutorials/Data/JetDataset/jetImage_7_100p_0_10000.h5']\n",
    "# if you are running locallt, you can use the full dataset doing\n",
    "# for fileIN in glob.glob(\"tutorials/HiggsSchool/data/*h5\"):\n",
    "for fileIN in datafiles:\n",
    "    print(\"Appending %s\" %fileIN)\n",
    "    f = h5py.File(fileIN)\n",
    "    myJetList = np.array(f.get(\"jetConstituentList\")[:,:,[5,8,11]])\n",
    "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
    "    jetList = np.concatenate([jetList, myJetList], axis=0) if jetList.size else myJetList\n",
    "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
    "    del myJetList, mytarget\n",
    "    f.close()\n",
    "print(target.shape, jetList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch Cross Entropy doesn't support one-hot encoding\n",
    "target = np.argmax(target, axis=1)\n",
    "# the dataset is N_jets x N_particles x N_features\n",
    "# the IN wants N_jets x N_features x N_particles\n",
    "jetList = np.swapaxes(jetList, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 50K with up to 100 particles in each jet. These 100 particles have been used to fill the 100x100 jet images.\n",
    "\n",
    "---\n",
    "\n",
    "We now shuffle the data, splitting them into a training and a validation dataset with 2:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nParticle = 30\n",
    "jetList = jetList[:,:,:nParticle]\n",
    "print(jetList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(jetList, target, test_size=0.33)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "del jetList, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a GPU is available. Otherwise run on CPU\n",
    "device = 'cpu'\n",
    "args_cuda = torch.cuda.is_available()\n",
    "if args_cuda: device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to pytorch\n",
    "X_train = Variable(torch.FloatTensor(X_train)).to(device)\n",
    "X_val = Variable(torch.FloatTensor(X_val)).to(device)\n",
    "y_train = Variable(torch.LongTensor(y_train).long()).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the IN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GraphNet, self).__init__()\n",
    "        \n",
    "        self.P = 3 # number of features\n",
    "        self.N = nParticle # number of particles\n",
    "        self.Nr = self.N * (self.N - 1)\n",
    "        self.De = 8 # dimensionality of De learned representation\n",
    "        self.Do = 8 # number of engineered features\n",
    "        self.n_targets = 5 # number of target classes\n",
    "        self.assign_matrices() # build Rr and Rs\n",
    "        \n",
    "        # a dense layer is allocated doing\n",
    "        # layer = nn.Linear(nodes_in, nodes_out).to(device)\n",
    "        # This computes y = w*x + b\n",
    "        # activation functions come later \n",
    "        self.fr1 =  # FILL THIS LINE making sure that the input dimension is correct\n",
    "        self.fr2 =  # FILL THIS LINE\n",
    "        self.fr3 =  # FILL THIS LINE making sure that the output dimension is correct\n",
    "        \n",
    "        self.fo1 = # FILL THIS LINE making sure that the input dimension is correct\n",
    "        self.fo2 = # FILL THIS LINE\n",
    "        self.fo3 = # FILL THIS LINE making sure that the output dimension is correct\n",
    "        \n",
    "        self.fc1 = # FILL THIS LINE making sure that the input dimension is correct\n",
    "        self.fc2 = # FILL THIS LINE\n",
    "        self.fc3 = # FILL THIS LINE making sure that the output dimension is correct\n",
    "             \n",
    "    def assign_matrices(self):\n",
    "        self.Rr = torch.zeros(self.N, self.Nr)\n",
    "        self.Rs = torch.zeros(self.N, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
    "        for i, (r, s) in enumerate(receiver_sender_list):\n",
    "            self.Rr[r, i] = 1\n",
    "            self.Rs[s, i] = 1\n",
    "        self.Rr = Variable(self.Rr)\n",
    "        self.Rs = Variable(self.Rs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # use Rr and Rs to create the B matrix\n",
    "        Orr = self.tmul(x, self.Rr)\n",
    "        Ors = self.tmul(x, self.Rs)\n",
    "        # This is how you append two tensors\n",
    "        # the 1 means that Rs is appended to Rr along the the second - vertical - axis \n",
    "        B = torch.cat([Orr, Ors], 1)\n",
    "        \n",
    "        ### First MLP ###\n",
    "        # this is how you transpose \n",
    "        # we transpose a lot. This is because we represent the input such\n",
    "        # that each column is an interation (or a message) between a sender and a receiver\n",
    "        # We want to pass *columns* to the dense NNs\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "        # the view command is a reshape function\n",
    "        B = nn.functional.relu(self.fr1(B.view(-1, 2 * self.P)))\n",
    "        B = # FILL THIS LINE\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "        del B\n",
    "        # now we transpose E back, so that interation are columns\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "        Ebar = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "        # we build C appending Ebar to x \n",
    "        C = # FILL THIS LINE\n",
    "        del Ebar\n",
    "        # again, we want to pass columns to the next networks.\n",
    "        # so we transpose\n",
    "        C = # FILL THIS LINE\n",
    "        ### Second MLP ###\n",
    "        C = nn.functional.relu(self.fo1(C.view(-1, self.P + self.De)))\n",
    "        C = # FILL THIS LINE\n",
    "        O = nn.functional.relu(self.fo3(C).view(-1, self.N, self.Do))\n",
    "        del C\n",
    "        # to build features, we apply an aggregation function \n",
    "        # you can use .sum, .max, etc (see https://pytorch.org/docs/stable/torch.html)\n",
    "        # the operation goes on the second axis, so you want to do torch.SOMETHING(O, 1)\n",
    "        O = torch.sum(O,1)\n",
    "        ### Classification MLP ###\n",
    "        N = # FILL THIS LINE\n",
    "        N = # FILL THIS LINE\n",
    "        del O\n",
    "        N = # FILL THIS LINE: remember that the loss function in pytorch applies the softmax function\n",
    "        return N\n",
    "\n",
    "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
    "        x_shape = x.size()\n",
    "        y_shape = y.size()\n",
    "        prod = torch.mm(x.reshape(x_shape[0]*x_shape[1], x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
    "        return prod\n",
    "\n",
    "def get_sample(training, target, choice):\n",
    "    target_vals = np.argmax(target, axis = 1)\n",
    "    ind, = np.where(target_vals == choice)\n",
    "    chosen_ind = np.random.choice(ind, 50000)\n",
    "    return training[chosen_ind], target[chosen_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up to 300 epochs (but we use early stopping)\n",
    "n_epochs = 300\n",
    "batch_size = 100\n",
    "# this is needed by early stopping\n",
    "patience =  10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GraphNet()\n",
    "gnn.to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gnn.parameters(), lr = 0.0001)\n",
    "\n",
    "loss_train = np.zeros(n_epochs)\n",
    "acc_train = np.zeros(n_epochs)\n",
    "loss_val = np.zeros(n_epochs)\n",
    "acc_val = np.zeros(n_epochs)\n",
    "for i in range(n_epochs):\n",
    "    print(\"Epoch %s\" % i)\n",
    "    for j in range(0, X_train.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        out = gnn(X_train[j:j + batch_size,:,:])\n",
    "        target = y_train[j:j + batch_size]\n",
    "        l = loss(out, target)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        loss_train[i] += l.cpu().data.numpy()*batch_size\n",
    "    loss_train[i] = loss_train[i]/X_train.shape[0]\n",
    "    #acc_train[i] = stats(predicted, Y_val)\n",
    "    #### val loss & accuracy\n",
    "    for j in range(0, X_val.size()[0], batch_size):\n",
    "        out_val = gnn(X_val[j:j + batch_size])\n",
    "        target_val =  y_val[j:j + batch_size]\n",
    "        \n",
    "        l_val = loss(out_val,target_val)\n",
    "        loss_val[i] += l_val.cpu().data.numpy()*batch_size\n",
    "    loss_val[i] = loss_val[i]/X_val.shape[0]\n",
    "    print(\"Training   Loss: %f\" %l.cpu().data.numpy())\n",
    "    print(\"Validation Loss: %f\" %l_val.cpu().data.numpy())\n",
    "    if all(loss_val[max(0, i - patience):i] > min(np.append(loss_val[0:max(0, i - patience)], 200))) and i > patience:\n",
    "        print(\"Early Stopping\")\n",
    "        break\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = list(range((loss_train > 0.).sum()))\n",
    "plt.figure()\n",
    "plt.plot(epoch_number, loss_train[loss_train>0.],label='Training Loss')\n",
    "plt.plot(epoch_number, loss_val[loss_train>0.],label='Validation Loss')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "#plt.savefig('%s/ROC.pdf'%(options.outputDir))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['gluon', 'quark', 'W', 'Z', 'top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "n_batches_val = int(X_val.size()[0]/batch_size)\n",
    "if args_cuda:    \n",
    "    for j in torch.split(X_val, n_batches_val).cuda():\n",
    "        a = gnn(j).cpu().data.numpy()\n",
    "        lst.append(a)\n",
    "else:\n",
    "    for j in torch.split(X_val, n_batches_val):\n",
    "        a = gnn(j).cpu().data.numpy()\n",
    "        lst.append(a)\n",
    "predicted = Variable(torch.FloatTensor(np.concatenate(lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no softmax in the output layer. We have to put it by \n",
    "predicted = torch.nn.functional.softmax(predicted, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_val = predicted.data.numpy()\n",
    "true_val = y_val.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "#### get the ROC curves\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "plt.figure()\n",
    "for i, label in enumerate(labels):\n",
    "        fpr[label], tpr[label], threshold = roc_curve((true_val== i), predict_val[:,i])\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "        plt.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "#plt.savefig('%s/ROC.pdf'%(options.outputDir))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

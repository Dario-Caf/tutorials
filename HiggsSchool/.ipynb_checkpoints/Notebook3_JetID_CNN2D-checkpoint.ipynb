{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "\n",
    "---\n",
    "In this notebook, we perform a Jet identification task using a Conv2D multiclass classifier.\n",
    "The problem consists on identifying a given jet as a quark, a gluon, a W, a Z, or a top,\n",
    "based on a jet image, i.e., a 2D histogram of the transverse momentum ($p_T$) deposited in each of 100x100\n",
    "bins of a square window of the ($\\eta$, $\\phi$) plane, centered along the jet axis.\n",
    "\n",
    "For details on the physics problem, see https://arxiv.org/pdf/1804.06913.pdf \n",
    "\n",
    "For details on the dataset, see Notebook1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the training and validation samples\n",
    "\n",
    "---\n",
    "Open the HDF5 files in the data/ directory and append them to a target and jetImage dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending data/jetImage_7_100p_80000_90000.h5\n",
      "Appending data/jetImage_7_100p_70000_80000.h5\n",
      "Appending data/jetImage_7_100p_0_10000.h5\n",
      "Appending data/jetImage_7_100p_10000_20000.h5\n",
      "Appending data/jetImage_7_100p_40000_50000.h5\n",
      "Appending data/jetImage_7_100p_60000_70000.h5\n",
      "Appending data/jetImage_7_100p_30000_40000.h5\n",
      "Appending data/jetImage_7_100p_50000_60000.h5\n",
      "(80000, 5) (80000, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "target = np.array([])\n",
    "jetImage = np.array([])\n",
    "for fileIN in glob.glob(\"data/jetImage_*_100p_*h5\"):\n",
    "    print(\"Appending %s\" %fileIN)\n",
    "    f = h5py.File(fileIN)\n",
    "    myjetImage = np.array(f.get(\"jetImage\"))\n",
    "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
    "    jetImage = np.concatenate([jetImage, myjetImage], axis=0) if jetImage.size else myjetImage\n",
    "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
    "print(target.shape, jetImage.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of XXX with up to 100 particles in each jet. These 100 particles have been used to fill the 100x100 jet images.\n",
    "\n",
    "---\n",
    "\n",
    "We now shuffle the data, splitting them into a training and a validation dataset with 2:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53600, 100, 100) (26400, 100, 100) (53600, 5) (26400, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(jetImage, target, test_size=0.33)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keras, images are representable as $n \\times m \\times k$ tensors, where $n \\times m$ are the pixel dimenions and $k$ is the number of channels (e.g., 1 in a black\\&while image, 3 for an RGB image). In our case, k=1. To comply to this, we add the channel index by reshaping the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53600, 100, 100, 1) (26400, 100, 100, 1) (53600, 5) (26400, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], X_val.shape[2], 1))\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv 2D model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# keras imports\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Conv2D, Dropout, Flatten\n",
    "from keras.layers import MaxPooling2D, BatchNormalization, Activation\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "dropoutRate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (img_rows, img_cols, 1)\n",
    "####\n",
    "inputImage = Input(shape=(image_shape))\n",
    "x = Conv2D(5, kernel_size=(5,5), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(inputImage)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D( pool_size = (5,5))(x)\n",
    "x = Dropout(dropoutRate)(x)\n",
    "#\n",
    "x = Conv2D(3, kernel_size=(3,3), data_format=\"channels_last\", strides=(1, 1), padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D( pool_size = (3,3))(x)\n",
    "x = Dropout(dropoutRate)(x)\n",
    "#\n",
    "x = Flatten()(x)\n",
    "#\n",
    "x = Dense(5, activation='relu')(x)\n",
    "#\n",
    "output = Dense(5, activation='softmax')(x)\n",
    "####\n",
    "model = Model(inputs=inputImage, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100, 100, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 100, 100, 5)       130       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100, 100, 5)       20        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100, 100, 5)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 20, 20, 5)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 20, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 20, 3)         138       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20, 20, 3)         12        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 20, 20, 3)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 3)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 3)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 545       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 859\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53600 samples, validate on 26400 samples\n",
      "Epoch 1/20\n",
      " - 205s - loss: 1.5708 - val_loss: 1.4752\n",
      "Epoch 2/20\n",
      " - 204s - loss: 1.4524 - val_loss: 1.3480\n",
      "Epoch 3/20\n",
      " - 205s - loss: 1.3530 - val_loss: 1.2450\n",
      "Epoch 4/20\n",
      " - 196s - loss: 1.2924 - val_loss: 1.2221\n",
      "Epoch 5/20\n",
      " - 192s - loss: 1.2673 - val_loss: 1.1997\n",
      "Epoch 6/20\n",
      " - 193s - loss: 1.2488 - val_loss: 1.1746\n",
      "Epoch 7/20\n",
      " - 193s - loss: 1.2414 - val_loss: 1.1701\n",
      "Epoch 8/20\n",
      " - 193s - loss: 1.2304 - val_loss: 1.1611\n",
      "Epoch 9/20\n",
      " - 209s - loss: 1.2246 - val_loss: 1.1552\n",
      "Epoch 10/20\n",
      " - 196s - loss: 1.2248 - val_loss: 1.1541\n",
      "Epoch 11/20\n",
      " - 212s - loss: 1.2162 - val_loss: 1.1491\n",
      "Epoch 12/20\n",
      " - 208s - loss: 1.2129 - val_loss: 1.1412\n",
      "Epoch 13/20\n",
      " - 224s - loss: 1.2100 - val_loss: 1.1471\n",
      "Epoch 14/20\n",
      " - 210s - loss: 1.2077 - val_loss: 1.1374\n",
      "Epoch 15/20\n",
      " - 2798s - loss: 1.2062 - val_loss: 1.1276\n",
      "Epoch 16/20\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, verbose = 2,\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
    "                TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.title('Training History')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save on disk the best model, result of the training, to be then use for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelName = 'Conv2D'\n",
    "model_json = model.to_json()\n",
    "with open(\"models/jetTagger_Conv2D.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"models/jetTagger_Conv2D.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['gluon', 'quark', 'W', 'Z', 'top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "predict_val = model.predict(X_val)\n",
    "df = pd.DataFrame()\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "\n",
    "plt.figure()\n",
    "for i, label in enumerate(labels):\n",
    "        df[label] = y_val[:,i]\n",
    "        df[label + '_pred'] = predict_val[:,i]\n",
    "\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        plt.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.000001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Anomalous Jet Detector with **RNN** \n",
    "\n",
    "---\n",
    "In this notebook, we train an unsupervised algorithm capable of compressing a jet image into a low-dimension laten space and, from there, reconstruct the input image. The distance between the input and te output is used to identify rare jet configurations. Applying a lower treshold on the loss, one can veto standard QCD jets (quarks and gluons) and select a sample enriched in anomalous jets (W, Z, top, etc). The model uses (De)Conv2D and Dense layers to process the image.\n",
    "\n",
    "This is based on the following papers:\n",
    "- https://arxiv.org/pdf/1808.08992.pdf\n",
    "- https://arxiv.org/pdf/1808.08979.pdf\n",
    "\n",
    "For details on the dataset, see Notebook1 and Notebook3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the training and validation samples\n",
    "\n",
    "---\n",
    "In order to import the dataset, we now\n",
    "- clone the dataset repository (to import the data in Colab)\n",
    "- load the h5 files in the data/ repository\n",
    "- extract the data we need: a target and jetImage \n",
    "\n",
    "To type shell commands, we start the command line with !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/pierinim/tutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls tutorials/Data/JetDataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array([])\n",
    "jetList = np.array([])\n",
    "# we cannot load all data on Colab. So we just take a few files\n",
    "datafiles = ['tutorials/Data/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
    "           'tutorials/Data/JetDataset/jetImage_7_100p_60000_70000.h5',\n",
    "            'tutorials/Data/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
    "            'tutorials/Data/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
    "            'tutorials/Data/JetDataset/jetImage_7_100p_0_10000.h5']\n",
    "# if you are running locallt, you can use the full dataset doing\n",
    "# for fileIN in glob.glob(\"tutorials/HiggsSchool/data/*h5\"):\n",
    "for fileIN in datafiles:\n",
    "    print(\"Appending %s\" %fileIN)\n",
    "    f = h5py.File(fileIN)\n",
    "    myJetList = np.array(f.get(\"jetConstituentList\"))\n",
    "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
    "    jetList = np.concatenate([jetList, myJetList], axis=0) if jetList.size else myJetList\n",
    "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
    "print(target.shape, jetList.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now separate the dataset in 4:\n",
    "- a training dataset, consisting of quarks and gluons\n",
    "- three 'anomalous jets' samples: W, Z, and top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetList_standard = jetList[np.argmax(target,axis=1)<2]\n",
    "jetList_W = jetList[np.argmax(target,axis=1)==2]\n",
    "jetList_Z = jetList[np.argmax(target,axis=1)==3]\n",
    "jetList_t = jetList[np.argmax(target,axis=1)==4]\n",
    "print(jetList_standard.shape, jetList_W.shape, jetList_Z.shape, jetList_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is an unsupervised algorithm, so we don't need the target array anymore.\n",
    "Nevertheless, we keep a part of it around, since it might be useful to test the response \n",
    "of the algorithm to quarks and gluons separetly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_standard = target[np.argmax(target,axis=1)<2]\n",
    "del target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now shuffle the standard-jet data and its labels, splitting them into a training, a validation+test dataset with 2:1:1 ratio. \n",
    "\n",
    "Then we separate the validation+test in two halves (training and validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, label_train, label_val = t= train_test_split(jetList_standard, label_standard, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, label_train.shape, label_val.shape)\n",
    "len_val = X_val.shape[0]\n",
    "X_test = X_val[int(len_val/2.):,:,:]\n",
    "label_test = label_val[int(len_val/2.):,:]\n",
    "X_val = X_val[:int(len_val/2.),:,:]\n",
    "label_test = label_val[:int(len_val/2.),:]\n",
    "print(X_train.shape, X_val.shape, X_test.shape, label_train.shape, label_val.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleanup to save memory\n",
    "del jetList, jetList_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RNN AE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, GRU, Activation\n",
    "from keras.layers import RepeatVector, Reshape\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureArrayLength = (X_train.shape[1],X_train.shape[2])\n",
    "dropoutRate = 0.25\n",
    "print(featureArrayLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------\n",
    "# Enncoder\n",
    "#---------\n",
    "inputList = Input(shape=(featureArrayLength))\n",
    "x = GRU(20, activation=\"tanh\", recurrent_activation='hard_sigmoid', return_sequences=False)(inputList)\n",
    "x = Dense(5, activation='relu')(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = RepeatVector(featureArrayLength[0])(x)\n",
    "outputList = GRU(16, activation=\"tanh\", recurrent_activation='hard_sigmoid', return_sequences=True, return_state=False)(x)\n",
    "####\n",
    "#\n",
    "autoencoder = Model(inputs=inputList, outputs=outputList)\n",
    "#\n",
    "#autoencoder = Model(inputs=inputList, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(loss='mse', optimizer='adam')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model. Notice the difference with respect to the supervised case\n",
    "- the input to the training is (X,X) and nor (X, y). Similarly for the validation dataset\n",
    "- the model has no dropout. It is difficult for an unsupervised model to overtran, so there is not really a need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "history = autoencoder.fit(X_train, X_train, epochs=n_epochs, batch_size=batch_size, verbose = 2,\n",
    "                validation_data=(X_val, X_val),\n",
    "                callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
    "                TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "#plt.yscale('log')\n",
    "plt.title('Training History')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['W', 'Z', 'top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = [jetList_W, jetList_Z, jetList_t]\n",
    "predictedQCD = autoencoder.predict(X_test)\n",
    "predicted_anomaly = []\n",
    "for i in range(len(labels)):\n",
    "    predicted_anomaly.append(autoencoder.predict(anomaly[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(List_in, List_out):\n",
    "    mse = (List_out-List_in)*(List_out-List_in)\n",
    "    # sum over features\n",
    "    mse = np.sum(mse,axis=-1)\n",
    "    # sum over particles\n",
    "    mse = np.sum(mse, axis=-1)\n",
    "    return mse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossQCD = mse(X_test, predictedQCD)\n",
    "loss_anomaly = []\n",
    "for i in range(len(labels)):\n",
    "    loss_anomaly.append(mse(anomaly[i], predicted_anomaly[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lossQCD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxScore = np.max(lossQCD)\n",
    "# plot QCD\n",
    "plt.figure()\n",
    "#plt.hist(lossQCD, bins=100, label='QCD', density=True, range=(0, maxScore), \n",
    "plt.hist(lossQCD, bins=100, label='QCD', density=True, \n",
    "         histtype='step', fill=False, linewidth=1.5)\n",
    "for i in range(len(labels)):\n",
    "    plt.hist(loss_anomaly[i], bins=100, label=labels[i], density=True, range=(0, maxScore),\n",
    "            histtype='step', fill=False, linewidth=1.5)\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"AE Loss\")\n",
    "plt.ylabel(\"Probability (a.u.)\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "plt.figure()\n",
    "targetQCD = np.zeros(lossQCD.shape[0])\n",
    "for i, label in enumerate(labels):\n",
    "        print(loss_anomaly[i].shape, targetQCD.shape)\n",
    "        trueVal = np.concatenate((np.ones(loss_anomaly[i].shape[0]),targetQCD))\n",
    "        predVal = np.concatenate((loss_anomaly[i],lossQCD))\n",
    "        print(trueVal.shape, predVal.shape)\n",
    "        fpr, tpr, threshold = roc_curve(trueVal,predVal)\n",
    "        auc1= auc(fpr, tpr)\n",
    "        plt.plot(tpr,fpr,label='%s Anomaly Detection, auc = %.1f%%'%(label,auc1*100.))\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
